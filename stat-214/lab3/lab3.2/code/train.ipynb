{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dcdb9fe-87b3-4a4f-aad1-db7f04f1418e",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "616fd306-e54b-422e-9059-1ca83eb86ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data import TextDataset\n",
    "from encoder import Encoder\n",
    "from train_encoder import train_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a472a5-5b94-4ca2-a4f8-1536c7ab20c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a9ab45-c63d-4c41-a6e7-28cc3a062d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "## open the data\n",
    "with open(\"../../../shared/data/raw_text.pkl\", \"rb\") as f:\n",
    "    raw_texts = pickle.load(f)\n",
    "\n",
    "# Combine all story content into one big list of strings\n",
    "all_texts = []\n",
    "\n",
    "# Pick a sample story\n",
    "sample_story = raw_texts['avatar']\n",
    "all_stories = {}  # Dict[story_id: str]\n",
    "\n",
    "for story_id, sequence in raw_texts.items():\n",
    "    try:\n",
    "        full_story = \" \".join(sequence.data)\n",
    "        all_stories[story_id] = full_story\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {story_id}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8a99308-be04-4925-b422-dd4bd40669b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_ids = list(all_stories.keys())\n",
    "\n",
    "# 2) split IDs\n",
    "train_ids, val_ids = train_test_split(\n",
    "    story_ids,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3) build lists of strings\n",
    "train_texts = [all_stories[sid] for sid in train_ids]\n",
    "val_texts   = [all_stories[sid] for sid in val_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db7dbc4-969e-4bfa-8614-90089b7e46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer    = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "train_ds      = TextDataset(train_texts, tokenizer, max_len=128)\n",
    "val_ds        = TextDataset(val_texts,   tokenizer, max_len=128)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=1, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce18b910-0e94-46ba-aa34-1388a4168ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      " Training with lr=0.0005,layers=2,hs=128\n",
      "Epoch 1/35 — train: 10.4352, val: 10.1911\n",
      "Epoch 2/35 — train: 10.0177, val: 10.0208\n",
      "Epoch 3/35 — train: 9.7289, val: 9.6310\n",
      "Epoch 4/35 — train: 9.3819, val: 9.3914\n",
      "Epoch 5/35 — train: 8.9530, val: 8.9530\n",
      "Epoch 6/35 — train: 8.7871, val: 8.7570\n",
      "Epoch 7/35 — train: 8.4488, val: 8.4374\n",
      "Epoch 8/35 — train: 8.2173, val: 8.2353\n",
      "Epoch 9/35 — train: 7.9796, val: 8.0041\n",
      "Epoch 10/35 — train: 7.6848, val: 7.5925\n",
      "Epoch 11/35 — train: 7.3822, val: 7.5015\n",
      "Epoch 12/35 — train: 7.1574, val: 7.5737\n",
      "Epoch 13/35 — train: 6.9396, val: 7.2475\n",
      "Epoch 14/35 — train: 6.7642, val: 7.3772\n",
      "Epoch 15/35 — train: 6.6847, val: 7.0580\n",
      "Epoch 16/35 — train: 6.5803, val: 7.2271\n",
      "Epoch 17/35 — train: 6.4136, val: 7.0179\n",
      "Epoch 18/35 — train: 6.4506, val: 6.9291\n",
      "Epoch 19/35 — train: 6.3766, val: 6.9859\n",
      "Epoch 20/35 — train: 6.4764, val: 7.4159\n",
      "Epoch 21/35 — train: 6.4955, val: 6.7978\n",
      "Epoch 22/35 — train: 6.2033, val: 6.8398\n",
      "Epoch 23/35 — train: 6.4447, val: 6.8933\n",
      "Epoch 24/35 — train: 6.1961, val: 6.6639\n",
      "Epoch 25/35 — train: 6.3636, val: 6.6342\n",
      "Epoch 26/35 — train: 6.3706, val: 7.0187\n",
      "Epoch 27/35 — train: 6.3231, val: 6.7816\n",
      "Epoch 28/35 — train: 6.2465, val: 7.1807\n",
      "Epoch 29/35 — train: 6.1491, val: 6.8767\n",
      "Epoch 30/35 — train: 6.1651, val: 7.1296\n",
      "Epoch 31/35 — train: 6.2762, val: 6.7336\n",
      "Epoch 32/35 — train: 6.1566, val: 7.4147\n",
      "Epoch 33/35 — train: 6.3769, val: 7.0966\n",
      "Epoch 34/35 — train: 6.2410, val: 7.0677\n",
      "Epoch 35/35 — train: 6.2508, val: 7.0771\n",
      "Saved model to: saved_models/encoder_lr0.0005_layers2_hs128.pt\n",
      "\n",
      " Training with lr=0.0005,layers=4,hs=256\n",
      "Epoch 1/35 — train: 10.1824, val: 9.7300\n",
      "Epoch 2/35 — train: 9.4165, val: 9.0238\n",
      "Epoch 3/35 — train: 8.8001, val: 8.5344\n",
      "Epoch 4/35 — train: 8.3839, val: 8.4423\n",
      "Epoch 5/35 — train: 7.8179, val: 7.7546\n",
      "Epoch 6/35 — train: 7.2565, val: 7.5093\n",
      "Epoch 7/35 — train: 6.9348, val: 7.3771\n",
      "Epoch 8/35 — train: 6.7656, val: 6.7710\n",
      "Epoch 9/35 — train: 6.6328, val: 7.0023\n",
      "Epoch 10/35 — train: 6.4926, val: 6.9917\n",
      "Epoch 11/35 — train: 6.6663, val: 7.0515\n",
      "Epoch 12/35 — train: 6.1570, val: 6.6100\n",
      "Epoch 13/35 — train: 6.3267, val: 6.9831\n",
      "Epoch 14/35 — train: 6.4516, val: 6.9764\n",
      "Epoch 15/35 — train: 6.3679, val: 7.3967\n",
      "Epoch 16/35 — train: 6.1814, val: 6.8069\n",
      "Epoch 17/35 — train: 6.2537, val: 6.9561\n",
      "Epoch 18/35 — train: 6.5283, val: 7.1162\n",
      "Epoch 19/35 — train: 6.1875, val: 7.4494\n",
      "Epoch 20/35 — train: 6.1888, val: 7.0105\n",
      "Epoch 21/35 — train: 6.1847, val: 6.8525\n",
      "Epoch 22/35 — train: 6.3291, val: 6.6798\n",
      "Epoch 23/35 — train: 6.0300, val: 7.4094\n",
      "Epoch 24/35 — train: 6.1641, val: 7.0607\n",
      "Epoch 25/35 — train: 6.1768, val: 6.8045\n",
      "Epoch 26/35 — train: 6.4103, val: 6.9120\n",
      "Epoch 27/35 — train: 6.2102, val: 7.0456\n",
      "Epoch 28/35 — train: 6.3144, val: 7.0009\n",
      "Epoch 29/35 — train: 6.0976, val: 7.2090\n",
      "Epoch 30/35 — train: 6.2912, val: 7.2511\n",
      "Epoch 31/35 — train: 6.1589, val: 7.2226\n",
      "Epoch 32/35 — train: 6.1899, val: 7.2955\n",
      "Epoch 33/35 — train: 5.9762, val: 7.0576\n",
      "Epoch 34/35 — train: 6.2537, val: 7.0085\n",
      "Epoch 35/35 — train: 6.1565, val: 6.7403\n",
      "Saved model to: saved_models/encoder_lr0.0005_layers4_hs256.pt\n",
      "\n",
      " Training with lr=0.0001,layers=4,hs=256\n",
      "Epoch 1/35 — train: 10.4333, val: 10.3339\n",
      "Epoch 2/35 — train: 10.1293, val: 10.0026\n",
      "Epoch 3/35 — train: 9.9395, val: 9.7101\n",
      "Epoch 4/35 — train: 9.7275, val: 9.6633\n",
      "Epoch 5/35 — train: 9.6305, val: 9.4459\n",
      "Epoch 6/35 — train: 9.4464, val: 9.3580\n",
      "Epoch 7/35 — train: 9.1801, val: 9.2824\n",
      "Epoch 8/35 — train: 9.1550, val: 9.1082\n",
      "Epoch 9/35 — train: 9.0780, val: 9.1081\n",
      "Epoch 10/35 — train: 8.8884, val: 8.7469\n",
      "Epoch 11/35 — train: 8.7421, val: 8.7939\n",
      "Epoch 12/35 — train: 8.5835, val: 8.5581\n",
      "Epoch 13/35 — train: 8.4478, val: 8.4553\n",
      "Epoch 14/35 — train: 8.2802, val: 8.4118\n",
      "Epoch 15/35 — train: 8.1593, val: 8.3645\n",
      "Epoch 16/35 — train: 8.1771, val: 8.4570\n",
      "Epoch 17/35 — train: 7.9762, val: 8.0016\n",
      "Epoch 18/35 — train: 7.7118, val: 8.2175\n",
      "Epoch 19/35 — train: 7.7309, val: 8.1730\n",
      "Epoch 20/35 — train: 7.5957, val: 7.8357\n",
      "Epoch 21/35 — train: 7.4559, val: 7.8437\n",
      "Epoch 22/35 — train: 7.4218, val: 7.6373\n",
      "Epoch 23/35 — train: 7.3327, val: 7.8513\n",
      "Epoch 24/35 — train: 7.1420, val: 7.5116\n",
      "Epoch 25/35 — train: 7.2544, val: 7.6119\n",
      "Epoch 26/35 — train: 6.9864, val: 7.3043\n",
      "Epoch 27/35 — train: 7.1638, val: 7.3058\n",
      "Epoch 28/35 — train: 7.0485, val: 7.5030\n",
      "Epoch 29/35 — train: 6.8462, val: 7.0508\n",
      "Epoch 30/35 — train: 6.8066, val: 7.0968\n",
      "Epoch 31/35 — train: 6.9229, val: 7.3134\n",
      "Epoch 32/35 — train: 6.7279, val: 7.1361\n",
      "Epoch 33/35 — train: 6.6934, val: 7.3351\n",
      "Epoch 34/35 — train: 6.7038, val: 6.8905\n",
      "Epoch 35/35 — train: 6.6809, val: 7.2100\n",
      "Saved model to: saved_models/encoder_lr0.0001_layers4_hs256.pt\n",
      "\n",
      " Training with lr=0.0001,layers=6,hs=512\n",
      "Epoch 1/35 — train: 10.2266, val: 9.7390\n",
      "Epoch 2/35 — train: 9.6532, val: 9.4919\n",
      "Epoch 3/35 — train: 9.2634, val: 9.0814\n",
      "Epoch 4/35 — train: 8.7912, val: 8.9078\n",
      "Epoch 5/35 — train: 8.7073, val: 8.7330\n",
      "Epoch 6/35 — train: 8.4251, val: 8.5224\n",
      "Epoch 7/35 — train: 8.1532, val: 8.4479\n",
      "Epoch 8/35 — train: 7.9951, val: 7.9626\n",
      "Epoch 9/35 — train: 7.8737, val: 7.6967\n",
      "Epoch 10/35 — train: 7.4583, val: 7.9370\n",
      "Epoch 11/35 — train: 7.3768, val: 7.6331\n",
      "Epoch 12/35 — train: 7.3511, val: 7.5473\n",
      "Epoch 13/35 — train: 7.0676, val: 7.6658\n",
      "Epoch 14/35 — train: 7.1027, val: 7.6450\n",
      "Epoch 15/35 — train: 6.9575, val: 7.5188\n",
      "Epoch 16/35 — train: 6.9044, val: 7.0166\n",
      "Epoch 17/35 — train: 6.7111, val: 6.7618\n",
      "Epoch 18/35 — train: 6.7871, val: 7.1786\n",
      "Epoch 19/35 — train: 6.5651, val: 7.1282\n",
      "Epoch 20/35 — train: 6.7796, val: 7.3603\n",
      "Epoch 21/35 — train: 6.3901, val: 6.9025\n",
      "Epoch 22/35 — train: 6.4403, val: 7.4619\n",
      "Epoch 23/35 — train: 6.3853, val: 6.7843\n",
      "Epoch 24/35 — train: 6.3971, val: 6.9702\n",
      "Epoch 25/35 — train: 6.3430, val: 6.9917\n",
      "Epoch 26/35 — train: 6.1979, val: 7.1299\n",
      "Epoch 27/35 — train: 6.4304, val: 7.2421\n",
      "Epoch 28/35 — train: 6.3365, val: 6.8505\n",
      "Epoch 29/35 — train: 6.3285, val: 6.7267\n",
      "Epoch 30/35 — train: 6.1188, val: 7.3373\n",
      "Epoch 31/35 — train: 6.2856, val: 7.1802\n",
      "Epoch 32/35 — train: 6.3686, val: 6.8716\n",
      "Epoch 33/35 — train: 6.3161, val: 6.9398\n",
      "Epoch 34/35 — train: 6.1652, val: 6.9889\n",
      "Epoch 35/35 — train: 5.9525, val: 7.1004\n",
      "Saved model to: saved_models/encoder_lr0.0001_layers6_hs512.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from encoder import Encoder\n",
    "from train_encoder import train_bert\n",
    "import os\n",
    "\n",
    "\n",
    "save_dir = \"saved_models\"\n",
    "os.makedirs(save_dir, exist_ok=True) \n",
    "\n",
    "# ─── 1) Define your hyper‑parameter grid ──────────────────────────────────────\n",
    "configs = [\n",
    "    {\"lr\": 5e-4, \"num_layers\": 2, \"hidden_size\": 128},\n",
    "    {\"lr\": 5e-4, \"num_layers\": 4, \"hidden_size\": 256},\n",
    "    {\"lr\": 1e-4, \"num_layers\": 4, \"hidden_size\": 256},\n",
    "    {\"lr\": 1e-4, \"num_layers\": 6, \"hidden_size\": 512},\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "results = {}  # Stores loss for each config\n",
    "\n",
    "\n",
    "# ─── 2) Loop over configs ─────────────────────────────────────────────────\n",
    "\n",
    "for cfg in configs:\n",
    "    config_str = f\"lr={cfg['lr']},layers={cfg['num_layers']},hs={cfg['hidden_size']}\"\n",
    "    print(f\"\\n Training with {config_str}\")\n",
    "\n",
    "    # Instantiate encoder with the cfg\n",
    "    model = Encoder(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        hidden_size=cfg[\"hidden_size\"],\n",
    "        num_heads=4,  # fixed for now\n",
    "        num_layers=cfg[\"num_layers\"],\n",
    "        intermediate_size=cfg[\"hidden_size\"] * 2,\n",
    "        max_len=128\n",
    "    ).to(device)\n",
    "\n",
    "    # Call training loop\n",
    "    model, train_losses, val_losses = train_bert(\n",
    "        model=model,\n",
    "        train_dataloader=train_loader,\n",
    "        val_dataloader=val_loader,\n",
    "        tokenizer=tokenizer,\n",
    "        epochs=35, #40, can be tuned\n",
    "        lr=cfg[\"lr\"],\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Store losses\n",
    "    results[config_str] = (train_losses, val_losses)\n",
    "\n",
    "    # Save model to saved_models folder\n",
    "    filename = f\"encoder_lr{cfg['lr']}_layers{cfg['num_layers']}_hs{cfg['hidden_size']}.pt\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Saved model to: {save_path}\")\n",
    "\n",
    "with open(\"mlm_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfba410e-4dfb-4776-8be3-bf00e86339a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mlm_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe4576a-96e1-4c54-95a2-3d04d66590fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: loss_plots/loss_lr_0_0005_layers_2_hs_128.png\n",
      "Saved: loss_plots/loss_lr_0_0005_layers_4_hs_256.png\n",
      "Saved: loss_plots/loss_lr_0_0001_layers_4_hs_256.png\n",
      "Saved: loss_plots/loss_lr_0_0001_layers_6_hs_512.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# ─── Load results from file ────────────────────────────────────────────────\n",
    "with open(\"mlm_results.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# ─── Create output directory ───────────────────────────────────────────────\n",
    "os.makedirs(\"loss_plots\", exist_ok=True)\n",
    "\n",
    "# ─── Plot each config separately ───────────────────────────────────────────\n",
    "for config_str, (train, val) in results.items():\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train, label=\"Train Loss\")\n",
    "    plt.plot(val, label=\"Validation Loss\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"Loss Curve — {config_str}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Format filename safely\n",
    "    safe_name = config_str.replace(\"=\", \"_\").replace(\",\", \"_\").replace(\".\", \"_\")\n",
    "    path = f\"loss_plots/loss_{safe_name}.png\"\n",
    "    plt.savefig(path)\n",
    "    plt.close()  # Close figure to avoid overlapping\n",
    "\n",
    "    print(f\"Saved: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9866abe9-5145-4773-85a2-8cbf9f120d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: loss_plots/loss_lr_0_0005_comparison.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load results\n",
    "with open(\"mlm_results.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "os.makedirs(\"loss_plots\", exist_ok=True)\n",
    "\n",
    "# Filter configs with lr = 5e-4\n",
    "configs_to_plot = [k for k in results if \"lr=0.0005\" in k]\n",
    "configs_to_plot.sort()  # consistent order\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 5), sharey=True)\n",
    "\n",
    "for ax, config_str in zip(axes, configs_to_plot):\n",
    "    train, val = results[config_str]\n",
    "    ax.plot(train, label=\"Train Loss\")\n",
    "    ax.plot(val, label=\"Validation Loss\", linestyle=\"--\")\n",
    "    ax.set_title(config_str.replace(\"lr=\", \"lr=\").replace(\",\", \"\\n\"))  # multiline title\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.grid(True)\n",
    "    if ax is axes[0]:\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "\n",
    "fig.suptitle(\"Training vs Validation Loss (lr=5e-4)\")\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.95])  # leave space for suptitle\n",
    "\n",
    "\n",
    "save_path = \"loss_plots/loss_lr_0_0005_comparison.png\"\n",
    "plt.savefig(save_path,dpi=600)\n",
    "plt.close()\n",
    "print(f\"Saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c447fc99-6894-4c11-98cb-8590d598eee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best model loss plot to: loss_plots/best_model_loss.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Load results\n",
    "with open(\"mlm_results.pkl\", \"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "os.makedirs(\"loss_plots\", exist_ok=True)\n",
    "\n",
    "# Target config (best model)\n",
    "target_config = \"lr=0.0001,layers=6,hs=512\"\n",
    "train, val = results[target_config]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train, label=\"Train Loss\")\n",
    "plt.plot(val, label=\"Validation Loss\", linestyle=\"--\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "plt.title(\"Best Model\\nlr=1e-4, layers=6, hidden=512\", fontsize=13)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "save_path = \"loss_plots/best_model_loss.png\"\n",
    "plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"Saved best model loss plot to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99798088-5680-487e-bce7-1d229603a817",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64626c72-3565-432a-91c4-3af795c865ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from encoder import Encoder\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from preprocessing import lanczosinterp2D, make_delayed\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c69b081-47ef-48b9-82cc-9ee603263684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real vocab size: 30522\n",
      "tokenizer vocab size: 30522\n",
      "model token embedding shape: torch.Size([30522, 512])\n"
     ]
    }
   ],
   "source": [
    "model_path = \"saved_models/encoder_lr0.0001_layers6_hs512.pt\" \n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "vocab_size = len(tokenizer.get_vocab()) \n",
    "print(\"Real vocab size:\", vocab_size)\n",
    "\n",
    "model = Encoder(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=512,\n",
    "    num_heads=4,\n",
    "    num_layers=6,\n",
    "    intermediate_size=1024,\n",
    "    max_len=128\n",
    ")\n",
    "model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "print(\"tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "print(\"model token embedding shape:\", model.token_emb.weight.shape)\n",
    "\n",
    "def extract_embeddings(seq, model, tokenizer, chunk_size=128, stride=64, hidden_size=512):\n",
    "    device = next(model.parameters()).device\n",
    "    text_words = seq.data\n",
    "    total_words = len(text_words)\n",
    "    word_embeddings = [None] * total_words\n",
    "\n",
    "    for start in range(0, total_words, stride):\n",
    "        chunk_words = text_words[start:start + chunk_size]\n",
    "        tokens = tokenizer(\n",
    "            chunk_words,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128, \n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        word_ids = tokens.word_ids(batch_index=0)\n",
    "\n",
    "        input_ids = tokens[\"input_ids\"].to(device)\n",
    "        token_type_ids = tokens[\"token_type_ids\"].to(device)\n",
    "        attention_mask = tokens[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            hidden_states = model(\n",
    "                input_ids=input_ids,\n",
    "                token_type_ids=token_type_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                return_hidden=True\n",
    "            )[0]\n",
    "\n",
    "        hidden_states = hidden_states.squeeze(0).cpu()\n",
    "\n",
    "        attention_mask = tokens[\"attention_mask\"][0]  # shape: (seq_len,)\n",
    "        word_ids = tokens.word_ids(batch_index=0)\n",
    "        \n",
    "        for token_idx, word_idx in enumerate(word_ids):\n",
    "            if word_idx is None:\n",
    "                continue\n",
    "            absolute_word_idx = start + word_idx\n",
    "            if absolute_word_idx >= total_words:\n",
    "                continue\n",
    "            if word_embeddings[absolute_word_idx] is None:\n",
    "                word_embeddings[absolute_word_idx] = []\n",
    "            word_embeddings[absolute_word_idx].append(hidden_states[token_idx])\n",
    "\n",
    "    for i in range(total_words):\n",
    "        if word_embeddings[i] is None:\n",
    "            word_embeddings[i] = torch.zeros(hidden_size)\n",
    "        else:\n",
    "            word_embeddings[i] = torch.stack(word_embeddings[i]).mean(0)\n",
    "\n",
    "    return torch.stack(word_embeddings).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38efa65c-483c-44ea-8bc7-cb980f1d95f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [01:58<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../../shared/data/raw_text.pkl\", \"rb\") as f:\n",
    "    raw_texts = pickle.load(f)\n",
    "\n",
    "for story_id, seq in tqdm(raw_texts.items()):\n",
    "    try:\n",
    "        emb = extract_embeddings(seq, model, tokenizer)\n",
    "        X_interp = lanczosinterp2D(\n",
    "            emb, oldtime=seq.data_times, newtime=seq.tr_times\n",
    "        )\n",
    "        TR = np.mean(np.diff(seq.tr_times))\n",
    "        n_skip_start = int(np.ceil(5 / TR))\n",
    "        n_skip_end = int(np.ceil(10 / TR))\n",
    "        X_interp = X_interp[n_skip_start:-n_skip_end]\n",
    "        X_delayed = make_delayed(X_interp, [1,2,3,4])\n",
    "        np.save(f\"../results/embeddings/encoder/{story_id}.npy\", X_delayed)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping {story_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed556ba-371f-4fd4-83cb-72575fa37acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
